{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático 02 - Boosting\n",
    "- Thiago Martin Poppe\n",
    "- 2017014324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo os dados\n",
    "\n",
    "- Leitura dos dados do dataset Tic-Tac-Toe, que representa uma possível configuração do tabuleiro de Jogo da Velha aonde o jogador X joga primeiro.<br><br>\n",
    "\n",
    "- Os dados possuem 9 features, sendo elas cada célula do Jogo da Velha.\n",
    "    - Cada feature assume 1 dentre 3 valores, podendo ser:\n",
    "        - x: o jogador X escolheu aquela posição.\n",
    "        - o: o jogador O escolheu aquela posição.\n",
    "        - b: posição vazia.<br><br>\n",
    "\n",
    "- Possuímos apenas 2 classes nesse dataset, sendo elas:\n",
    "    - positive: o jogador X ganha a partida.\n",
    "    - negative: o jogador X perde a partida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_lft</th>\n",
       "      <th>top_mid</th>\n",
       "      <th>top_rgt</th>\n",
       "      <th>mid_lft</th>\n",
       "      <th>mid_mid</th>\n",
       "      <th>mid_rgt</th>\n",
       "      <th>btm_lft</th>\n",
       "      <th>btm_mid</th>\n",
       "      <th>btm_rgt</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>958 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    top_lft top_mid top_rgt mid_lft mid_mid mid_rgt btm_lft btm_mid btm_rgt  \\\n",
       "0         x       x       x       x       o       o       x       o       o   \n",
       "1         x       x       x       x       o       o       o       x       o   \n",
       "2         x       x       x       x       o       o       o       o       x   \n",
       "3         x       x       x       x       o       o       o       b       b   \n",
       "4         x       x       x       x       o       o       b       o       b   \n",
       "5         x       x       x       x       o       o       b       b       o   \n",
       "6         x       x       x       x       o       b       o       o       b   \n",
       "7         x       x       x       x       o       b       o       b       o   \n",
       "8         x       x       x       x       o       b       b       o       o   \n",
       "9         x       x       x       x       b       o       o       o       b   \n",
       "10        x       x       x       x       b       o       o       b       o   \n",
       "11        x       x       x       x       b       o       b       o       o   \n",
       "12        x       x       x       o       x       o       x       o       o   \n",
       "13        x       x       x       o       x       o       o       x       o   \n",
       "14        x       x       x       o       x       o       o       o       x   \n",
       "15        x       x       x       o       x       o       o       b       b   \n",
       "16        x       x       x       o       x       o       b       o       b   \n",
       "17        x       x       x       o       x       o       b       b       o   \n",
       "18        x       x       x       o       x       b       o       o       b   \n",
       "19        x       x       x       o       x       b       o       b       o   \n",
       "20        x       x       x       o       x       b       b       o       o   \n",
       "21        x       x       x       o       o       x       x       o       o   \n",
       "22        x       x       x       o       o       x       o       x       o   \n",
       "23        x       x       x       o       o       x       o       o       x   \n",
       "24        x       x       x       o       o       x       o       b       b   \n",
       "25        x       x       x       o       o       x       b       o       b   \n",
       "26        x       x       x       o       o       x       b       b       o   \n",
       "27        x       x       x       o       o       b       x       o       b   \n",
       "28        x       x       x       o       o       b       x       b       o   \n",
       "29        x       x       x       o       o       b       o       x       b   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "928       b       b       x       x       x       b       o       o       o   \n",
       "929       b       b       x       x       b       x       o       o       o   \n",
       "930       b       b       x       o       o       o       x       x       b   \n",
       "931       b       b       x       o       o       o       x       b       x   \n",
       "932       b       b       x       o       o       o       b       x       x   \n",
       "933       b       b       x       b       x       x       o       o       o   \n",
       "934       b       b       o       x       x       o       x       b       o   \n",
       "935       b       b       o       x       x       o       b       x       o   \n",
       "936       b       b       o       x       o       x       o       x       b   \n",
       "937       b       b       o       x       o       x       o       b       x   \n",
       "938       b       b       o       x       o       b       o       x       x   \n",
       "939       b       b       o       x       b       o       x       x       o   \n",
       "940       b       b       o       b       x       o       x       x       o   \n",
       "941       b       b       o       b       o       x       o       x       x   \n",
       "942       x       x       o       o       x       x       x       o       o   \n",
       "943       x       x       o       o       o       x       x       x       o   \n",
       "944       x       x       o       o       o       x       x       o       x   \n",
       "945       x       o       x       x       x       o       o       x       o   \n",
       "946       x       o       x       x       o       x       o       x       o   \n",
       "947       x       o       x       x       o       o       o       x       x   \n",
       "948       x       o       x       o       x       x       o       x       o   \n",
       "949       x       o       x       o       o       x       x       x       o   \n",
       "950       x       o       o       o       x       x       x       x       o   \n",
       "951       o       x       x       x       x       o       o       o       x   \n",
       "952       o       x       x       x       o       o       x       o       x   \n",
       "953       o       x       x       x       o       o       o       x       x   \n",
       "954       o       x       o       x       x       o       x       o       x   \n",
       "955       o       x       o       x       o       x       x       o       x   \n",
       "956       o       x       o       o       x       x       x       o       x   \n",
       "957       o       o       x       x       x       o       o       x       x   \n",
       "\n",
       "        class  \n",
       "0    positive  \n",
       "1    positive  \n",
       "2    positive  \n",
       "3    positive  \n",
       "4    positive  \n",
       "5    positive  \n",
       "6    positive  \n",
       "7    positive  \n",
       "8    positive  \n",
       "9    positive  \n",
       "10   positive  \n",
       "11   positive  \n",
       "12   positive  \n",
       "13   positive  \n",
       "14   positive  \n",
       "15   positive  \n",
       "16   positive  \n",
       "17   positive  \n",
       "18   positive  \n",
       "19   positive  \n",
       "20   positive  \n",
       "21   positive  \n",
       "22   positive  \n",
       "23   positive  \n",
       "24   positive  \n",
       "25   positive  \n",
       "26   positive  \n",
       "27   positive  \n",
       "28   positive  \n",
       "29   positive  \n",
       "..        ...  \n",
       "928  negative  \n",
       "929  negative  \n",
       "930  negative  \n",
       "931  negative  \n",
       "932  negative  \n",
       "933  negative  \n",
       "934  negative  \n",
       "935  negative  \n",
       "936  negative  \n",
       "937  negative  \n",
       "938  negative  \n",
       "939  negative  \n",
       "940  negative  \n",
       "941  negative  \n",
       "942  negative  \n",
       "943  negative  \n",
       "944  negative  \n",
       "945  negative  \n",
       "946  negative  \n",
       "947  negative  \n",
       "948  negative  \n",
       "949  negative  \n",
       "950  negative  \n",
       "951  negative  \n",
       "952  negative  \n",
       "953  negative  \n",
       "954  negative  \n",
       "955  negative  \n",
       "956  negative  \n",
       "957  negative  \n",
       "\n",
       "[958 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\n",
    "    \"top_lft\", \"top_mid\", \"top_rgt\",\n",
    "    \"mid_lft\", \"mid_mid\", \"mid_rgt\",\n",
    "    \"btm_lft\", \"btm_mid\", \"btm_rgt\",\n",
    "    \n",
    "    \"class\"\n",
    "]\n",
    "\n",
    "data = pd.read_table(\"tic-tac-toe.data\", sep=\",\", names=columns)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive classes: 626\n",
      "Number of negative classes: 332\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of positive classes:\", (data[\"class\"] == \"positive\").sum())\n",
    "print(\"Number of negative classes:\", (data[\"class\"] == \"negative\").sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validação Cruzada\n",
    "- Usaremos a classe KFold, disponível em ``sklearn.model_selection``, para realizar a validação cruzada com 5-folds.<br><br>\n",
    "\n",
    "- Note que temos um desbalanceamento das classes no dataset. Temos aproximadamente o dobro de classes positivas em relação às classes negativas, que além disso estão todas juntas.<br><br>\n",
    "\n",
    "- Devido a esse motivo, julguei importante utilizar KFold com ``shuffle=True`` para garantir que as classes estejam misturadas durante o treino e validação. Fixei a semente em 42 para que o mesmo comportamento se repita em outras execuções do Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds = 5\n"
     ]
    }
   ],
   "source": [
    "# Retriving X and y\n",
    "X = data.drop(\"class\", axis=1)\n",
    "y = data[\"class\"].replace([\"positive\", \"negative\"], [1, -1]) # y must be either 1 or -1\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"Number of folds =\", kf.get_n_splits())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe Stump\n",
    "\n",
    "- Um Stump, ou Decision Stump, é um modelo de classificação fraco que consiste em ser basicamente uma Árvore de Decisão com altura igual à 1 e 2 folhas. Com isso, conseguimos deduzir de onde veio esse nome, visto que Stump é uma tradução literal de tronco de árvore.<br><br>\n",
    "\n",
    "- Modelei um Stump como uma classe para facilitar o uso do mesmo ao longo do algoritmo do AdaBoost. Nela, teremos 3 parâmetros onde 2 podem ser opcionais:\n",
    "    - feature: representa sobre qual feature estaremos aplicando o Stump.\n",
    "    - value: representa o valor daquela feature. No caso do dataset poderá ser x, o ou b.\n",
    "    - pred: representa qual predição será dada caso o valor da feature seja value.<br><br>\n",
    "    \n",
    "- Por exemplo, ao instanciarmos um Stump(0, x, 1) todos os valores iguais à x da feature 0 serão classificados como da classe 1 e classificados como a classe -1 caso contrário.<br><br>\n",
    "\n",
    "- Além das features podemos passar TRUE e FALSE, como strings, para um Stump. Nesse caso, as predições serão sempre 1 caso o Stump seja inicializado com TRUE e -1 caso seja inicializado com FALSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stump:\n",
    "    \"\"\"\n",
    "        Implementation of Stump Model - weak classifier\n",
    "        A Stump is a decision tree with 1 node and 2 leafs only\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature, value=None, pred=None):\n",
    "        \"\"\"\n",
    "            Constructor of class\n",
    "            Needs a feature, value and prediction associated with that feature\n",
    "            \n",
    "            Feature can also be TRUE or FALSE, in that case value=None and pred=None by default\n",
    "        \"\"\"\n",
    "        self._pred = pred\n",
    "        self._value = value\n",
    "        self._feature = feature\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "            For printing a Stump\n",
    "        \"\"\"\n",
    "        return \"(%s, %s, %s)\" % (self._feature, self._value, self._pred)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            Class prediction by model (feature, value, pred)\n",
    "            Returns a numpy.ndarray with values -pred and pred\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        if self._feature == \"TRUE\": # all classes are 1\n",
    "            return np.ones(n_samples).astype(int)\n",
    "        \n",
    "        if self._feature == \"FALSE\": # all classes are -1\n",
    "            return (-1) * np.ones(n_samples).astype(int)\n",
    "        \n",
    "        preds = np.empty(n_samples).astype(int)\n",
    "        for i in range(n_samples):\n",
    "            if X[i, self._feature] == self._value:\n",
    "                preds[i] = self._pred\n",
    "            else:\n",
    "                preds[i] = (-1) * self._pred\n",
    "                \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando a tabela de Stumps\n",
    "\n",
    "- Para gerar cada Stump, criamos 3 laços variando as features, valores e predições a serem passadas para cada Stump. Posteriormente, criaremos dois Stumps \"gerais\" onde todas as predições são 1 e todas as predições são -1.<br><br>\n",
    "\n",
    "- No total teremos $9 * 3 * 2 + 2 = 56$ Stumps gerados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stump 1 = (0, x, -1)\n",
      "Stump 2 = (0, x, 1)\n",
      "Stump 3 = (0, o, -1)\n",
      "Stump 4 = (0, o, 1)\n",
      "Stump 5 = (0, b, -1)\n",
      "Stump 6 = (0, b, 1)\n",
      "Stump 7 = (1, x, -1)\n",
      "Stump 8 = (1, x, 1)\n",
      "Stump 9 = (1, o, -1)\n",
      "Stump 10 = (1, o, 1)\n",
      "Stump 11 = (1, b, -1)\n",
      "Stump 12 = (1, b, 1)\n",
      "Stump 13 = (2, x, -1)\n",
      "Stump 14 = (2, x, 1)\n",
      "Stump 15 = (2, o, -1)\n",
      "Stump 16 = (2, o, 1)\n",
      "Stump 17 = (2, b, -1)\n",
      "Stump 18 = (2, b, 1)\n",
      "Stump 19 = (3, x, -1)\n",
      "Stump 20 = (3, x, 1)\n",
      "Stump 21 = (3, o, -1)\n",
      "Stump 22 = (3, o, 1)\n",
      "Stump 23 = (3, b, -1)\n",
      "Stump 24 = (3, b, 1)\n",
      "Stump 25 = (4, x, -1)\n",
      "Stump 26 = (4, x, 1)\n",
      "Stump 27 = (4, o, -1)\n",
      "Stump 28 = (4, o, 1)\n",
      "Stump 29 = (4, b, -1)\n",
      "Stump 30 = (4, b, 1)\n",
      "Stump 31 = (5, x, -1)\n",
      "Stump 32 = (5, x, 1)\n",
      "Stump 33 = (5, o, -1)\n",
      "Stump 34 = (5, o, 1)\n",
      "Stump 35 = (5, b, -1)\n",
      "Stump 36 = (5, b, 1)\n",
      "Stump 37 = (6, x, -1)\n",
      "Stump 38 = (6, x, 1)\n",
      "Stump 39 = (6, o, -1)\n",
      "Stump 40 = (6, o, 1)\n",
      "Stump 41 = (6, b, -1)\n",
      "Stump 42 = (6, b, 1)\n",
      "Stump 43 = (7, x, -1)\n",
      "Stump 44 = (7, x, 1)\n",
      "Stump 45 = (7, o, -1)\n",
      "Stump 46 = (7, o, 1)\n",
      "Stump 47 = (7, b, -1)\n",
      "Stump 48 = (7, b, 1)\n",
      "Stump 49 = (8, x, -1)\n",
      "Stump 50 = (8, x, 1)\n",
      "Stump 51 = (8, o, -1)\n",
      "Stump 52 = (8, o, 1)\n",
      "Stump 53 = (8, b, -1)\n",
      "Stump 54 = (8, b, 1)\n",
      "Stump 55 = (TRUE, None, None)\n",
      "Stump 56 = (FALSE, None, None)\n"
     ]
    }
   ],
   "source": [
    "stumps_table = []\n",
    "\n",
    "# Creating all possible stumps\n",
    "# feature -> columns of X\n",
    "for feature in range(X.shape[1]):\n",
    "    for value in ['x', 'o', 'b']:\n",
    "        for pred in [-1, 1]:\n",
    "            stumps_table.append(Stump(feature, value, pred))\n",
    "\n",
    "# Creating both TRUE and FALSE stumps\n",
    "stumps_table.append(Stump(\"TRUE\"))\n",
    "stumps_table.append(Stump(\"FALSE\"))\n",
    "\n",
    "# Showing stumps\n",
    "for i, stump in enumerate(stumps_table):\n",
    "    print(\"Stump %d = %s\" % (i+1, stump))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe AdaBoost\n",
    "- AdaBoost é um modelo de aprendizagem de máquina da família ``ensemble``. Ele se baseia na técnica de Boosting utilizando Stumps como classificadores mais fracos. Através destes, o Boosting tenta minimizar o alto viés associado a cada modelo dando a resposta como uma combinação das respostas destes classificadores mais fracos. No caso, cada classificador carregará consigo um peso, indicando o quão influente ele será na resposta final. <br><br>\n",
    "\n",
    "- A cada iteração do algoritmo, o modelo irá escolher aquele Stump que melhor minimiza o erro empírico dado que cada amostra possui um peso associado com ela. Inicialmente, todas as amostras possuem pesos iguais, de tal forma que seu somatório seja igual à 1. Após cada iteração, esses pesos são atualizados de forma que o próximo modelo escolhido não erre em dados que os modelos anteriores acertaram. Em outras palavras, erros passados influenciam em escolhas futuras.<br><br>\n",
    "\n",
    "- Algo interessante do AdaBoost é que ele é robusto à overfitting. Isso pode ser explicado visto que cada Stump é impossível de sofrer overfitting, devido à sua extrema simplicidade. Os Stumps apenas são um pouco melhores do que jogar uma moeda para cima e decidir a classe de uma amostra baseada no seu resultado. Unido ao fato anterior, o AdaBoost consegue obter resultados melhores quanto mais iterações fizermos no algoritmo, como veremos nas análises no final desse Notebook.<br><br>\n",
    "\n",
    "- Com isso, o treinamento do AdaBoost implementado na classe será dado pelo algoritmo:\n",
    "    - Notação.: Usaremos $h(X)$ para representar a predição dos modelos fracos; $H(X)$ para a predição do modelo forte; $\\alpha$ para os pesos associados com cada modelo fraco; $w_i$ para os pesos associados com cada amostra $i$.\n",
    "    1. Inicialize todos os pesos $w_i$ com valor $\\frac{1}{N}$, onde $N$ corresponde ao número de amostras dos dados.\n",
    "    2. Para cada iteração faça:\n",
    "        - A) Escolha o Stump que melhor minimiza o erro empírico dado por: $\\normalsize erro = \\sum\\limits_i w_i$, $\\forall i \\mid h(X_i) \\neq y_i$.\n",
    "        - B) Compute o peso desse Stump através da fórmula: $\\normalsize \\alpha = 0.5 * ln(\\frac{1 - erro}{erro})$.\n",
    "        - C) Atualize os pesos: $\\normalsize w_i = w_i * e^{-\\alpha * h(X) * y}$.\n",
    "        - D) Normalize os pesos: $\\normalsize w = \\frac{w}{\\sum_i w_i}$, com isso a soma dos novos pesos será 1.\n",
    "        - E) Salve os valores de $\\alpha$ e o Stump escolhido.<br><br>\n",
    "        \n",
    "- Como o modelo final é um conjunto formado por $t$ classificadores mais fracos, a predição final será dada por uma combinação linear dos pesos de cada classificador com a predição dos mesmos. Se a predição final for um valor menor que 0 retornamos -1; caso contrário retornamos 1. Com isso, teremos que: $$ H(X) = sign(\\sum\\limits_{i=1}^t \\alpha_i h_i(X)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \"\"\"\n",
    "        Implementation of AdaBoost ensemble model\n",
    "        It uses Stump classes as weak classifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stumps_table, debug=False):\n",
    "        \"\"\"\n",
    "            Constructor of class\n",
    "            Needs a premade Stumps Table to be used by the model\n",
    "        \"\"\" \n",
    "        self._alphas  = None\n",
    "        self._weights = None\n",
    "        self._stumps  = None\n",
    "        self._stumps_table = stumps_table\n",
    "        \n",
    "    def _pick_best_stump(self, X, y):\n",
    "        \"\"\"\n",
    "            This function picks the Stump in the Stumps Table that best\n",
    "            minimizes the error given sample weights.\n",
    "\n",
    "            Returns the best Stump model, it's error and predictions\n",
    "        \"\"\"    \n",
    "        best_error = np.inf\n",
    "        best_model = None\n",
    "        best_preds = None\n",
    "\n",
    "        for stump in self._stumps_table:\n",
    "            preds = stump.predict(X)\n",
    "            error = self._weights[(preds != y)].sum()\n",
    "\n",
    "            if error < best_error:\n",
    "                best_model = stump\n",
    "                best_error  = error\n",
    "                best_preds = preds\n",
    "                \n",
    "        return best_model, best_error, best_preds\n",
    "    \n",
    "    def print_stumps(self):\n",
    "        \"\"\"\n",
    "            Only a debug method to print Stumps chose by model\n",
    "            with it's alpha values\n",
    "        \"\"\"\n",
    "        n_stumps = len(self._stumps)\n",
    "        for i in range(n_stumps):\n",
    "            print(\"- Weak classifier %d:\" % (i+1))\n",
    "            print(\"  - Alpha = %.5f\" % self._alphas[i])\n",
    "            print(\"  - Stump =\", self._stumps[i])\n",
    "            print()\n",
    "        \n",
    "    def fit(self, X, y, n_iter):\n",
    "        \"\"\"\n",
    "            Fits model to data\n",
    "            y values must be either 1 or -1\n",
    "            n_iter defines how many iterations the algorithm will execute\n",
    "        \"\"\"\n",
    "        if set(y) != {-1, 1}:\n",
    "            raise ValueError(\"y values must be either 1 or -1\")\n",
    "            \n",
    "        if n_iter <= 0:\n",
    "            raise ValueError(\"The number of iterations must be greater than 0\")\n",
    "            \n",
    "        n_samples = X.shape[0]\n",
    "        self._stumps  = np.empty(n_iter, dtype=Stump)\n",
    "        self._alphas  = np.empty(n_iter)\n",
    "        self._weights = np.ones(n_samples) / n_samples # in the beginning all weights are 1/n\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            stump, error, preds = self._pick_best_stump(X, y)\n",
    "            \n",
    "            # Computing alpha value for that Stump \n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            \n",
    "            # Computing next weights and normalizing\n",
    "            weights = self._weights * np.exp(-alpha * preds * y)\n",
    "            weights /= weights.sum()\n",
    "            \n",
    "            # Saving current iteration values\n",
    "            self._stumps[i] = stump\n",
    "            self._alphas[i] = alpha\n",
    "            self._weights = weights\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            Class prediction by model given by dot(alphas, stumps prediction)\n",
    "            Returns a numpy.ndarray with values -1 and 1\n",
    "        \"\"\"\n",
    "        preds = np.array([stump.predict(X) for stump in self._stumps])\n",
    "        return np.sign(np.dot(self._alphas, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando o modelo\n",
    "\n",
    "- Treinaremos o modelo AdaBoost variando o hiperparâmetro referente ao número de iterações realizadas pelo algoritmo. O número de iterações está estritamente ligado com o número de Stumps a ser usado pelo modelo, visto que a cada iteração o algoritmo escolhe aquele Stump que melhor minimiza o erro empírico, aumentando em 1 o número de Stumps usados.<br><br>\n",
    "\n",
    "- Salvaremos as acurácias de treino e validação de cada modelo em um dicionário para ser usado em análises posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#iter = 1, train_acc = 0.699372, val_acc = 0.699346\n",
      "#iter = 3, train_acc = 0.658406, val_acc = 0.648091\n",
      "#iter = 5, train_acc = 0.744255, val_acc = 0.744197\n",
      "#iter = 7, train_acc = 0.721286, val_acc = 0.717065\n",
      "#iter = 9, train_acc = 0.738245, val_acc = 0.728578\n",
      "#iter = 11, train_acc = 0.747904, val_acc = 0.741089\n",
      "#iter = 13, train_acc = 0.761736, val_acc = 0.759855\n",
      "#iter = 15, train_acc = 0.775046, val_acc = 0.767168\n",
      "#iter = 17, train_acc = 0.781570, val_acc = 0.773402\n",
      "#iter = 19, train_acc = 0.784965, val_acc = 0.770288\n",
      "#iter = 21, train_acc = 0.792794, val_acc = 0.772393\n",
      "#iter = 23, train_acc = 0.790443, val_acc = 0.771346\n",
      "#iter = 25, train_acc = 0.784443, val_acc = 0.772377\n",
      "#iter = 27, train_acc = 0.784966, val_acc = 0.780726\n",
      "#iter = 29, train_acc = 0.791228, val_acc = 0.782788\n",
      "#iter = 31, train_acc = 0.794880, val_acc = 0.789054\n",
      "#iter = 33, train_acc = 0.799582, val_acc = 0.798457\n",
      "#iter = 35, train_acc = 0.813674, val_acc = 0.809926\n",
      "#iter = 37, train_acc = 0.814197, val_acc = 0.799493\n",
      "#iter = 39, train_acc = 0.821764, val_acc = 0.801603\n",
      "#iter = 41, train_acc = 0.820718, val_acc = 0.793254\n",
      "#iter = 43, train_acc = 0.816545, val_acc = 0.794284\n",
      "#iter = 45, train_acc = 0.821501, val_acc = 0.818308\n",
      "#iter = 47, train_acc = 0.820456, val_acc = 0.808890\n",
      "#iter = 49, train_acc = 0.828027, val_acc = 0.818276\n",
      "#iter = 51, train_acc = 0.830633, val_acc = 0.825611\n",
      "#iter = 53, train_acc = 0.845249, val_acc = 0.836027\n",
      "#iter = 55, train_acc = 0.841337, val_acc = 0.834997\n",
      "#iter = 57, train_acc = 0.846292, val_acc = 0.834997\n",
      "#iter = 59, train_acc = 0.855686, val_acc = 0.832913\n",
      "#iter = 61, train_acc = 0.856211, val_acc = 0.837118\n",
      "#iter = 63, train_acc = 0.851776, val_acc = 0.835019\n",
      "#iter = 65, train_acc = 0.856471, val_acc = 0.845446\n",
      "#iter = 67, train_acc = 0.859601, val_acc = 0.844394\n",
      "#iter = 69, train_acc = 0.860904, val_acc = 0.845452\n",
      "#iter = 71, train_acc = 0.868734, val_acc = 0.861098\n",
      "#iter = 73, train_acc = 0.874478, val_acc = 0.864229\n",
      "#iter = 75, train_acc = 0.872649, val_acc = 0.864223\n",
      "#iter = 77, train_acc = 0.875260, val_acc = 0.857962\n",
      "#iter = 79, train_acc = 0.871084, val_acc = 0.852776\n",
      "#iter = 81, train_acc = 0.881522, val_acc = 0.862173\n",
      "#iter = 83, train_acc = 0.879956, val_acc = 0.863209\n",
      "#iter = 85, train_acc = 0.884134, val_acc = 0.865292\n",
      "#iter = 87, train_acc = 0.888312, val_acc = 0.869459\n",
      "#iter = 89, train_acc = 0.893271, val_acc = 0.878834\n",
      "#iter = 91, train_acc = 0.901620, val_acc = 0.894514\n",
      "#iter = 93, train_acc = 0.900057, val_acc = 0.884064\n",
      "#iter = 95, train_acc = 0.902928, val_acc = 0.883001\n",
      "#iter = 97, train_acc = 0.913891, val_acc = 0.885089\n",
      "#iter = 99, train_acc = 0.908661, val_acc = 0.884086\n",
      "#iter = 101, train_acc = 0.908667, val_acc = 0.888258\n",
      "#iter = 103, train_acc = 0.911278, val_acc = 0.892430\n",
      "#iter = 105, train_acc = 0.915712, val_acc = 0.899744\n",
      "#iter = 107, train_acc = 0.919366, val_acc = 0.903921\n",
      "#iter = 109, train_acc = 0.924584, val_acc = 0.909146\n",
      "#iter = 111, train_acc = 0.931894, val_acc = 0.918526\n",
      "#iter = 113, train_acc = 0.927978, val_acc = 0.908077\n",
      "#iter = 115, train_acc = 0.932938, val_acc = 0.911224\n",
      "#iter = 117, train_acc = 0.937116, val_acc = 0.912255\n",
      "#iter = 119, train_acc = 0.940243, val_acc = 0.912293\n",
      "#iter = 121, train_acc = 0.937897, val_acc = 0.919574\n",
      "#iter = 123, train_acc = 0.937374, val_acc = 0.913302\n",
      "#iter = 125, train_acc = 0.941811, val_acc = 0.919574\n",
      "#iter = 127, train_acc = 0.941808, val_acc = 0.930034\n",
      "#iter = 129, train_acc = 0.953291, val_acc = 0.927940\n",
      "#iter = 131, train_acc = 0.957211, val_acc = 0.940445\n",
      "#iter = 133, train_acc = 0.955644, val_acc = 0.936267\n",
      "#iter = 135, train_acc = 0.956948, val_acc = 0.933153\n",
      "#iter = 137, train_acc = 0.960078, val_acc = 0.934211\n",
      "#iter = 139, train_acc = 0.963470, val_acc = 0.941514\n",
      "#iter = 141, train_acc = 0.956949, val_acc = 0.935237\n",
      "#iter = 143, train_acc = 0.955379, val_acc = 0.931054\n",
      "#iter = 145, train_acc = 0.955642, val_acc = 0.933164\n",
      "#iter = 147, train_acc = 0.960600, val_acc = 0.946733\n",
      "#iter = 149, train_acc = 0.967642, val_acc = 0.953027\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "models = dict()\n",
    "\n",
    "# Number of iterations from 1 to 150 with increment of 2\n",
    "for i in range(1, 151, 2):    \n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    # Dividing data into train and validation (5 folds)\n",
    "    for train, val in kf.split(X, y):\n",
    "        clf = AdaBoost(stumps_table)\n",
    "        clf.fit(X[train], y[train], n_iter=i)\n",
    "\n",
    "        # Computing train accuracy\n",
    "        preds = clf.predict(X[train])\n",
    "        train_acc = accuracy_score(y[train], preds)\n",
    "        train_acc_list.append(train_acc)\n",
    "\n",
    "        # Computing validation accuracy\n",
    "        preds = clf.predict(X[val])\n",
    "        val_acc = accuracy_score(y[val], preds)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "    # Computing average train and validation acurracy\n",
    "    models[i] = {\n",
    "        \"train_acc\": np.mean(train_acc_list),\n",
    "        \"val_acc\":   np.mean(val_acc_list)\n",
    "    }\n",
    "    \n",
    "    print(\"#iter = %d, train_acc = %f, val_acc = %f\" % (i, models[i][\"train_acc\"], models[i][\"val_acc\"]))\n",
    "    \n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados gráficos\n",
    "- Exibindo graficamente a evolução das acurácias de treino e validação com o aumento do número de iterações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = models.keys()\n",
    "train_acc = [acc[\"train_acc\"] for acc in models.values()]\n",
    "val_acc   = [acc[\"val_acc\"] for acc in models.values()]\n",
    "\n",
    "# Labels of plot\n",
    "plt.title(\"AdaBoost Model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "\n",
    "# Plotting train and validation accuracy\n",
    "plt.plot(x, train_acc, label='Train')\n",
    "plt.plot(x, val_acc, label='Validation')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- De acordo com os resultados obtidos acima, podemos perceber que o resultado foi muito bom. Observamos que o aumento de iterações reflete em um aumento da acurácia de forma significativa. Se utilizarmos 20 Stumps o resultado é bem pior do que se usarmos 100 Stumps, por exemplo. <br><br>\n",
    "\n",
    "- Note que as curvas de validação e treino estão próximas uma das outras, mostrando que não caímos em overfitting, mesmo com um modelo mais complexo, i.e com mais iterações. Isso pode ser explicado devido ao uso de Stumps pelo AdaBoost. Como cada modelo fraco nunca irá sofrer overfitting, por ser extremamente simples, o modelo forte (ensemble) também não sofrerá desse problema.<br><br>\n",
    "\n",
    "- Para resultados ainda melhores poderíamos ter treinado o modelo por mais iterações. Porém, como esse código foi feito em Python sem nenhuma otimização, como por exemplo o uso da biblioteca Numba, iríamos consumir muito tempo e ter resultados similiares com o que foi mostrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
